{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration of House Price Predictions\n",
    "\n",
    "\n",
    "The data is downloaded from Kaggle and contains data about houses and around ~80 variables. The goal is to use these explanatory variables to predict the House Prices. Here, we are dealing with a regression problem. As we want to conduct a classification, as well, we will also divide the SalePrice in three categories, namely \"low\", \"middle\" and \"upper class\". We will then attempt to predict these 3 classes. \n",
    "\n",
    "https://www.kaggle.com/c/house-prices-advanced-regression-techniques\n",
    "\n",
    "This notebook is to explore the data, to understand the basic relationships between the variables and to get a feeling about which variables might be good predictors for the House prices. There will be a separate notebook containing statistical and machine learning models for the predictions.\n",
    "\n",
    "Author: Julia Hammerer, Vanessa Mai\n",
    "Last Changes: 18.11.2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Data-exploration-of-House-Price-Predictions\" data-toc-modified-id=\"Data-exploration-of-House-Price-Predictions-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Data exploration of House Price Predictions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data-Profile\" data-toc-modified-id=\"Data-Profile-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Data Profile</a></span></li><li><span><a href=\"#Missing-values\" data-toc-modified-id=\"Missing-values-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Missing values</a></span></li><li><span><a href=\"#Sanity-checks\" data-toc-modified-id=\"Sanity-checks-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Sanity checks</a></span></li><li><span><a href=\"#Cleanse-data\" data-toc-modified-id=\"Cleanse-data-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Cleanse data</a></span></li><li><span><a href=\"#Explorations\" data-toc-modified-id=\"Explorations-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Explorations</a></span><ul class=\"toc-item\"><li><span><a href=\"#Correlation-Analysis-for-numeric-features\" data-toc-modified-id=\"Correlation-Analysis-for-numeric-features-1.5.1\"><span class=\"toc-item-num\">1.5.1&nbsp;&nbsp;</span>Correlation Analysis for numeric features</a></span></li><li><span><a href=\"#Categorical-Features\" data-toc-modified-id=\"Categorical-Features-1.5.2\"><span class=\"toc-item-num\">1.5.2&nbsp;&nbsp;</span>Categorical Features</a></span></li></ul></li><li><span><a href=\"#Explorations-based-on-self-defined-categories\" data-toc-modified-id=\"Explorations-based-on-self-defined-categories-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Explorations based on self-defined categories</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../helper/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load packages\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pandas_profiling\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "import warnings\n",
    "import math\n",
    "\n",
    "from pandas.tools.plotting import table\n",
    "from plotly.offline import init_notebook_mode\n",
    "from plotly.offline import iplot\n",
    "from plotly.offline import plot\n",
    "from scipy.stats import mannwhitneyu\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import norm\n",
    "\n",
    "from helper import na_ratio_table\n",
    "from helper import corr_heatmap\n",
    "from helper import corr_matrix_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "# we have two files, since this is a part of a kaggle competition,\n",
    "# only the training-set contains the target variable\n",
    "# we will use that for the whole analysis\n",
    "\n",
    "df=pd.read_csv(\"../data/house_prices_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of records and variables: \",df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for a first overview, we apply the pandas-profile report\n",
    "# it provides simple histograms, distributions, missingness \n",
    "# and correlations for all variables\n",
    "\n",
    "pandas_profiling.ProfileReport(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A description of all data fields can be found on the Kaggle site: https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data. Most of them are self-explanatory though.\n",
    "\n",
    "Around half the variables are categorical and the other half are numerical. For the categorical variables there will be need to use hot-one-encoding for incorporating them into the prediction models.\n",
    "\n",
    "We can already spot some correlations that look promising. Some of them\n",
    "also are also expected and won't give us further insights. We are particularly\n",
    "interested in correlations with our target variable\n",
    "- OverallQual - SalePrice\n",
    "- GrLivArea - SalePrice\n",
    "- FullBath - SalePrice\n",
    "- GarageYrBlt - YearRemodAdd\n",
    "- LotFrontage - lotArea\n",
    "- TotRmsAbvGrd - GrLivArea\n",
    "- BsmtUnfSF - BsmtFinSF1: negative correlation\n",
    "\n",
    "Also, we can detect variables that probably won't be of much use\n",
    "e.g.\n",
    "- Street: only two values, of which one is extremly low.\n",
    "- Utilities: Almost constant with two values, of which the other one has only one record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(include=['object']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values\n",
    "Let's check the missingness in more detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(na_ratio_table(df)[na_ratio_table(df)[\"NA_COUNT\"]>0])\n",
    "display(na_ratio_table(df)[na_ratio_table(df)[\"NA_COUNT\"]>0].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 19 variables that contain missing values. Most of them mean that the feature is simply not available for that property. However for a few, this can indicate a data quality issue:\n",
    "- Electrical: the type is not stated, it is improbable that there is no electrical system at all. \n",
    "- LotFrontage: a building should always have a lotfrontage\n",
    "\n",
    "As for \"Electrical\" only one record is missing, we can simply filter this out, or even ignore this. For the LotFrontage we can apply some imputation-techniques if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we test if the data is randomly missing, or if there are some patterns in the missingness\n",
    "# this helps us indicate whether there are data quality issues or if the missingness is part of the data\n",
    "msno.heatmap(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we can see that some of the variables are always missing together, which makes absolutely sense. \n",
    "Example: All Garage related variables are always missing together. Reason: no garage -> no values for any garage features.\n",
    "The other group of variables missing together is related to the basement. Because of these correlations, we might run into some multicollinearity issues in the modelling part later on.  \"Multicollinearity is a state of very high intercorrelations or inter-associations among the independent variables. It is therefore a type of disturbance in the data, and if present in the data the statistical inferences made about the data may not be reliable.\" (https://www.statisticssolutions.com/multicollinearity/, Accesed on: 22.11.2018) <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity checks\n",
    "We're going to check if there are some inconsistencies in the data or duplicates, etc. (Quality assessment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# any duplicates?\n",
    "df[df.duplicated(keep=False)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# any built year before sold year?\n",
    "df.query('YearBuilt > YrSold')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanse data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove useless columns\n",
    "df=df.drop(columns=[\"Id\", \"Street\", \"Utilities\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the missing record for Electrical\n",
    "df=df[df[\"Electrical\"].isna()==False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's once again check the distribution of the SalePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df[\"SalePrice\"], hist=False, label=\"SalePrice\", fit=norm,  kde_kws={\"shade\": True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution is slightly skewed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.probplot(df['SalePrice'], plot=plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SalePrice does not have a normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Analysis for numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we check for further correlations using different plots\n",
    "NUM_FEATURES =df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "df_num=df[NUM_FEATURES]\n",
    "\n",
    "df_corr=df_num.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_heatmap(df_corr, figsize=(20, 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix_1(df_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This visualization is not suitable due to the great amount of variables. Let's filter out the ones with the highest correlations and visualize those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_corr=df_corr.abs()\n",
    "\n",
    "df_corr=(df_corr.where(~np.tril(np.ones(df_corr.shape)).astype(np.bool)))\n",
    "\n",
    "df_high_corr=(df_corr[df_corr>0.75].dropna(how=\"all\", axis=0).dropna(how=\"all\", axis=1))\n",
    "\n",
    "df_high_corr_vars=np.unique(np.concatenate((df_high_corr.columns.values, df_high_corr.index.values)))\n",
    "\n",
    "df_high_corr = df[df_high_corr_vars]\n",
    "\n",
    "df_high_corr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix_1(df_high_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr.iloc[:,-1].sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variables which seem to have a high correlation with the target variable are also amongst the ones with the highest correlations in general.\n",
    "Let's also have a look at some of the categorical variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAT_FEATURES =df.select_dtypes(include=\"object\").columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(40,400))\n",
    "\n",
    "for i, a in enumerate(CAT_FEATURES):\n",
    "#     plt.figure(figsize=(5,2))\n",
    "    plt.subplot(math.ceil(len(CAT_FEATURES)),2,((i+1)*2-1))\n",
    "    sns.boxplot(x=\"SalePrice\", y=a, data=df)\n",
    "    plt.ylabel(a, fontsize=40)\n",
    "    plt.tick_params(axis='y', which='major', labelsize=30)\n",
    "\n",
    "    plt.subplot(math.ceil(len(CAT_FEATURES)),2,((i+1)*2))\n",
    "    for b in df[a].unique():\n",
    "        sns.distplot(df[df[a]==b][\"SalePrice\"], hist=False, label=a,  kde_kws={\"shade\": True})\n",
    "#     plt.ylabel(a, fontsize=40)\n",
    "\n",
    "#     plt.tight_layout()\n",
    "# sns.distplot(tsh_sta_pcu.query('Label_trunc == \"PCU1\"')['Wdf'], hist=False, label='PCU1', kde_kws={\"shade\": True})\n",
    "# sns.distplot(tsh_sta_pcu.query('Label_trunc == \"PCU2\"')['Wdf'], hist=False, label='PCU2', kde_kws={\"shade\": True});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a feeling of which attributes might be more important and can help us predict the SalePrice.\n",
    "For these attributes for example the SalePrice is different based on the value:\n",
    "* Alley\n",
    "* Neighborhood\n",
    "* ExterQual\n",
    "* ExterCond\n",
    "* KitchenQual\n",
    "* PoolQual\n",
    "\n",
    "For BsmtFinType2 e.g. the SalePrice cannot be differentiated much. Hence, this attribute might have less predictive power compared to some of the other attriutes.\n",
    "\n",
    "The EDA here already shows us potential to predict the SalePrice. Definitively, there are some relationship towards the SalePrice. Next step is to built models to predict the SalePrice and investigate the significance of differences in the attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorations based on self-defined categories\n",
    "As mentioned in the beginning, we will also divide the SalePrice in three categories, namely \"low\", \"middle\" and \"upper class\". Thus, we also want to see how these three can be separated and if there is some obvious differences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df[\"SalePrice\"], hist=True, label=\"SalePrice\", fit=norm,  kde_kws={\"shade\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"SalePrice\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"SalePrice\"].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df.SalePrice, bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The range of the SalePrice is from 34,900 - 755,000. The middle class would be the most frequent one. We decided to use the following boundaries:\n",
    "- low: 0-100,000\n",
    "- middle: 100,001-400,000\n",
    "- upper: from 400,001 onwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new column\n",
    "def price_class(row):\n",
    "    if row[\"SalePrice\"]<=120000:\n",
    "        return \"low\"\n",
    "    elif (row[\"SalePrice\"]>120000)& (row[\"SalePrice\"]<=250000):\n",
    "        return \"middle\"\n",
    "    else:\n",
    "        return\"upper\"\n",
    "\n",
    "\n",
    "df[\"Price_Class\"]= df.apply(lambda row: price_class(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(40,400))\n",
    "\n",
    "for i, a in enumerate(NUM_FEATURES):\n",
    "#     plt.figure(figsize=(5,2))\n",
    "    plt.subplot(math.ceil(len(CAT_FEATURES)),2,((i+1)*2-1))\n",
    "    sns.boxplot(y=a, x=\"Price_Class\", data=df)\n",
    "    plt.ylabel(a, fontsize=40)\n",
    "    plt.tick_params(axis='y', which='major', labelsize=30)\n",
    "\n",
    "    plt.subplot(math.ceil(len(CAT_FEATURES)),2,((i+1)*2))\n",
    "    for b in df[\"Price_Class\"].unique():\n",
    "        sns.distplot(df[df[\"Price_Class\"]==b][a], hist=False, label=b,  kde_kws={\"shade\": True})\n",
    "#     plt.ylabel(a, fontsize=40)\n",
    "\n",
    "#     plt.tight_layout()\n",
    "# sns.distplot(tsh_sta_pcu.query('Label_trunc == \"PCU1\"')['Wdf'], hist=False, label='PCU1', kde_kws={\"shade\": True})\n",
    "# sns.distplot(tsh_sta_pcu.query('Label_trunc == \"PCU2\"')['Wdf'], hist=False, label='PCU2', kde_kws={\"shade\": True});"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "367px",
    "left": "1549px",
    "right": "20px",
    "top": "119px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
