{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Analysis on House Prices\n",
    "\n",
    "We will divide the house prices in three categories and then try to predict those\n",
    "\n",
    "Author: Julia Hammerer, Vanessa Mai \n",
    "Last Changes: 10.12.2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split, KFold\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import itertools\n",
    "from scipy import interp\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"../data/house_prices_train.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\"Id\",\"Street\",\"Utilities\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new column\n",
    "def price_class(row):\n",
    "    if row[\"SalePrice\"]<=120000:\n",
    "        return \"low\"\n",
    "    elif (row[\"SalePrice\"]>120000)& (row[\"SalePrice\"]<=250000):\n",
    "        return \"middle\"\n",
    "    else:\n",
    "        return\"upper\"\n",
    "\n",
    "\n",
    "df[\"Price_Class\"]= df.apply(lambda row: price_class(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.replace({\"low\":0,\"middle\":1,\"upper\":2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.fillna(0)\n",
    "df.drop(columns=[\"SalePrice\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have to encode our categorical data\n",
    "cols_to_transform= df.drop(columns=[\"Price_Class\"]).select_dtypes(include='object').columns.values\n",
    "df_hot1encoded= pd.get_dummies(df, columns=cols_to_transform, prefix=cols_to_transform, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_hot1encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hot1encoded= df_hot1encoded.fillna(0)\n",
    "display(df_hot1encoded.head())\n",
    "print(df_hot1encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureCols=df_hot1encoded.columns.tolist()\n",
    "featureCols.remove(\"Price_Class\")\n",
    "\n",
    "targetCol=df_hot1encoded[\"Price_Class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= df_hot1encoded[featureCols].values\n",
    "y= df_hot1encoded[\"Price_Class\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scaler= StandardScaler()\n",
    "scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison\n",
    "in order to compare the models, we will save the models and the CV-results in a list:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_models = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Iteration 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe1 = Pipeline([('stdSc', StandardScaler()),\n",
    "                 ('clf', LogisticRegression(C=0.1,random_state=1)) \n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=pipe1.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Accuracy: %.3f' % pipe1.score(X_train, y_train)) \n",
    "print('Test Accuracy: %.3f' % pipe1.score(X_test, y_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "print(confusion_mat)\n",
    "plot_confusion_matrix(confusion_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Accuracy:       \",accuracy_score(y_test, y_pred))\n",
    "print (\"Precision:      \",precision_score(y_test, y_pred,average=None))\n",
    "print (\"Recall:         \",recall_score(y_test, y_pred,average=None))\n",
    "print (\"F1-Score:       \",f1_score(y_test, y_pred,average=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results for the `Middle Class`is better than the others. This is expected, since there are more samples for that class. Overall, the results are somewhat good. Let's try optimizing it all a bit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Iteration 2\n",
    "Gridsearch plus CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe1.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since newton-cg and saga only accepts l2 penalty, we will only use that in the gridsearch\n",
    "penalties=[\"l2\"]\n",
    "c=np.logspace(-4, 0, 40)\n",
    "solvers = [\"newton-cg\", \"sag\", \"saga\", \"lbfgs\"]\n",
    "\n",
    "param_grid_lr={'clf__penalty' : penalties,\n",
    "            'clf__C' :c,\n",
    "              'clf__solver': solvers}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_randSearch = RandomizedSearchCV(estimator=pipe1, param_distributions=param_grid_lr,scoring='accuracy',cv=10, n_iter=8)\n",
    "lr_randSearch.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Configuration found by Grid Search\")\n",
    "print(lr_randSearch.best_score_)\n",
    "print(lr_randSearch.best_params_)\n",
    "print(lr_randSearch.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe2=lr_randSearch.best_estimator_\n",
    "pipe2.fit(X_train, y_train)\n",
    "y_pred=pipe2.predict(X_test)\n",
    "confusion_mat=confusion_matrix(y_test, y_pred)\n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "print(\"Recall: \",recall_score(y_test, y_pred,average=None))\n",
    "print(\"Precision: \",precision_score(y_test, y_pred,average=None))\n",
    "print(\"F1: \",f1_score(y_test, y_pred,average=None))\n",
    "print(confusion_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add model to summary\n",
    "opt_models.append((\"Log Reg\", pipe2.steps[1][1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall is not as good as the other scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score=cross_val_score(pipe2, X, y, scoring=\"accuracy\", cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score.mean()\n",
    "print(\"Mean accuracy: %5.2f (+/-%5.2f) \"%(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ('Low', 'Middle', 'Upper')\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, title='Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance (LR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LRImp=pd.DataFrame(index=featureCols, columns=[\"LR\"])\n",
    "LRImp[\"LR\"]=pipe2.steps[1][1].coef_[0]\n",
    "feature_imp=LRImp.sort_values(by=\"LR\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp.head(10).sort_values(by=\"LR\", ascending=True).plot(kind=\"barh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp.tail(10).sort_values(by=\"LR\", ascending=True).plot(kind=\"barh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Iteration 3\n",
    "- used subset of feature based on previous Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_features=feature_imp[abs(feature_imp[\"LR\"])>0.05].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe3 = Pipeline([('stdSc', StandardScaler()),\n",
    "                 ('clf', LogisticRegression(C=0.1,random_state=1)) \n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since newton-cg and saga only accepts l2 penalty, we will only use that in the gridsearch\n",
    "penalties=[\"l2\"]\n",
    "c=np.logspace(-4, 0, 40)\n",
    "solvers = [\"newton-cg\", \"sag\", \"saga\", \"lbfgs\"]\n",
    "\n",
    "param_grid_lr={'clf__penalty' : penalties,\n",
    "            'clf__C' :c,\n",
    "              'clf__solver': solvers}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr_randSearch2 = RandomizedSearchCV(estimator=pipe3, param_distributions=param_grid_lr,scoring='accuracy',cv=10, n_iter=8)\n",
    "lr_randSearch2.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Configuration found by Grid Search\")\n",
    "print(lr_randSearch2.best_score_)\n",
    "print(lr_randSearch2.best_params_)\n",
    "print(lr_randSearch2.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe3=lr_randSearch.best_estimator_\n",
    "pipe3.fit(X_train, y_train)\n",
    "y_pred=pipe3.predict(X_test)\n",
    "confusion_mat=confusion_matrix(y_test, y_pred)\n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "print(\"Recall: \",recall_score(y_test, y_pred,average=None))\n",
    "print(\"Precision: \",precision_score(y_test, y_pred,average=None))\n",
    "print(\"F1: \",f1_score(y_test, y_pred,average=None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ('Low', 'Middle', 'Upper')\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, title='Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model did not improve much. Model can differentiate between low and high very well. For distinguishing between middle and low or middle and upper it has a little more difficulty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_svm1 = Pipeline([('stdSc', StandardScaler()),\n",
    "                 ('clf', svm.SVC() )\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_svm1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=pipe_svm1.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Accuracy: %.3f' % pipe_svm1.score(X_train, y_train)) \n",
    "print('Test Accuracy: %.3f' % pipe_svm1.score(X_test, y_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_mat = confusion_matrix(y_test, y_pred)\n",
    "plot_confusion_matrix(cnf_mat,class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Accuracy:       \",accuracy_score(y_test, y_pred))\n",
    "print (\"Precision:      \",precision_score(y_test, y_pred,average=None))\n",
    "print (\"Recall:         \",recall_score(y_test, y_pred,average=None))\n",
    "print (\"F1-Score:       \",f1_score(y_test, y_pred,average=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM iteration 2\n",
    "Let's try optimizing this model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_svm1.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_svm = [\n",
    " {'clf__C': [1, 10, 100, 1000], 'clf__kernel': ['linear']},\n",
    " {'clf__C': [1, 10, 100, 1000], 'clf__gamma': [0.001, 0.0001], 'clf__kernel': ['rbf']},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_GridSearch = GridSearchCV(estimator=pipe_svm1, param_grid=param_grid_svm,scoring='accuracy',cv=10)\n",
    "svm_GridSearch.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Configuration found by Grid Search\")\n",
    "print(svm_GridSearch.best_score_)\n",
    "print(svm_GridSearch.best_params_)\n",
    "print(svm_GridSearch.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_svm2=lr_randSearch.best_estimator_\n",
    "pipe_svm2.fit(X_train, y_train)\n",
    "y_pred=pipe_svm2.predict(X_test)\n",
    "confusion_mat=confusion_matrix(y_test, y_pred)\n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "print(\"Recall: \",recall_score(y_test, y_pred,average=None))\n",
    "print(\"Precision: \",precision_score(y_test, y_pred,average=None))\n",
    "print(\"F1: \",f1_score(y_test, y_pred,average=None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ('Low', 'Middle', 'Upper')\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, title='Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add model to summary\n",
    "opt_models.append((\"SVM\", pipe_svm2.steps[1][1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=7\n",
    "# evaluate each model in turn\n",
    "opt_results = []\n",
    "opt_names = []\n",
    "scoring = 'accuracy'\n",
    "for name, model in opt_models:\n",
    "    kfold = KFold(n_splits=10, random_state=seed)\n",
    "    cv_results = cross_val_score(model, X, y, cv=kfold, scoring=scoring)\n",
    "    opt_results.append(cv_results)\n",
    "    opt_names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "# boxplot algorithm comparison\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(opt_results)\n",
    "ax.set_xticklabels(opt_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Logistic Regression performs much better in a cross-validated comparison"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "192px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
