{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Analysis on House Prices\n",
    "\n",
    "After exploring the data and already finding some apparent relationships, we are going to build some regression models to predict the house prices. \n",
    "\n",
    "Author: Julia Hammerer, Vanessa Mai <br>\n",
    "Last Changes: 09.12.2018\n",
    "\n",
    "We can also have a look at the Kaggle Leaderboard for comparison:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Kaggle](../images/Kaggle_Leaderboard.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Regression-Analysis-on-House-Prices\" data-toc-modified-id=\"Regression-Analysis-on-House-Prices-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Regression Analysis on House Prices</a></span><ul class=\"toc-item\"><li><span><a href=\"#Function-Definitions\" data-toc-modified-id=\"Function-Definitions-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Function Definitions</a></span></li><li><span><a href=\"#Data-Preparation\" data-toc-modified-id=\"Data-Preparation-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Data Preparation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Create-Training-and-Testingset\" data-toc-modified-id=\"Create-Training-and-Testingset-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Create Training and Testingset</a></span></li><li><span><a href=\"#Normalize-Data-with-Standardscaler\" data-toc-modified-id=\"Normalize-Data-with-Standardscaler-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>Normalize Data with Standardscaler</a></span></li><li><span><a href=\"#Handle-outliers\" data-toc-modified-id=\"Handle-outliers-1.2.3\"><span class=\"toc-item-num\">1.2.3&nbsp;&nbsp;</span>Handle outliers</a></span></li></ul></li><li><span><a href=\"#Baseline-Model\" data-toc-modified-id=\"Baseline-Model-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Baseline Model</a></span></li><li><span><a href=\"#Ordinary-Least-Square-Regression-Model-(Iteration-1)\" data-toc-modified-id=\"Ordinary-Least-Square-Regression-Model-(Iteration-1)-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Ordinary Least Square Regression Model (Iteration 1)</a></span></li><li><span><a href=\"#Ordinary-Least-Square-Regression-Model-(Iteration-2)\" data-toc-modified-id=\"Ordinary-Least-Square-Regression-Model-(Iteration-2)-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Ordinary Least Square Regression Model (Iteration 2)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Residuals-vs.-Fitted\" data-toc-modified-id=\"Residuals-vs.-Fitted-1.5.1\"><span class=\"toc-item-num\">1.5.1&nbsp;&nbsp;</span>Residuals vs. Fitted</a></span></li></ul></li><li><span><a href=\"#Ordinary-Least-Square-Regression-Model-(Iteration-3)\" data-toc-modified-id=\"Ordinary-Least-Square-Regression-Model-(Iteration-3)-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Ordinary Least Square Regression Model (Iteration 3)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Residuals-vs.-Fitted\" data-toc-modified-id=\"Residuals-vs.-Fitted-1.6.1\"><span class=\"toc-item-num\">1.6.1&nbsp;&nbsp;</span>Residuals vs. Fitted</a></span></li></ul></li><li><span><a href=\"#Ordinary-Least-Square---Cross-Validation\" data-toc-modified-id=\"Ordinary-Least-Square---Cross-Validation-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>Ordinary Least Square - Cross Validation</a></span></li><li><span><a href=\"#Ordinary-Least-Square-Iteration-4\" data-toc-modified-id=\"Ordinary-Least-Square-Iteration-4-1.8\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>Ordinary Least Square Iteration 4</a></span></li><li><span><a href=\"#General-Least-Square-Regression\" data-toc-modified-id=\"General-Least-Square-Regression-1.9\"><span class=\"toc-item-num\">1.9&nbsp;&nbsp;</span>General Least Square Regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Residuals-vs.-Fitted\" data-toc-modified-id=\"Residuals-vs.-Fitted-1.9.1\"><span class=\"toc-item-num\">1.9.1&nbsp;&nbsp;</span>Residuals vs. Fitted</a></span></li></ul></li><li><span><a href=\"#Lasso-Regression-Iteration-1\" data-toc-modified-id=\"Lasso-Regression-Iteration-1-1.10\"><span class=\"toc-item-num\">1.10&nbsp;&nbsp;</span>Lasso Regression Iteration 1</a></span></li><li><span><a href=\"#Lasso-Regression-Iteration-2\" data-toc-modified-id=\"Lasso-Regression-Iteration-2-1.11\"><span class=\"toc-item-num\">1.11&nbsp;&nbsp;</span>Lasso Regression Iteration 2</a></span></li><li><span><a href=\"#Lasso-Regression-Iteration-3\" data-toc-modified-id=\"Lasso-Regression-Iteration-3-1.12\"><span class=\"toc-item-num\">1.12&nbsp;&nbsp;</span>Lasso Regression Iteration 3</a></span></li><li><span><a href=\"#Lasso-Regression-Iteration-4\" data-toc-modified-id=\"Lasso-Regression-Iteration-4-1.13\"><span class=\"toc-item-num\">1.13&nbsp;&nbsp;</span>Lasso Regression Iteration 4</a></span></li><li><span><a href=\"#Lasso-Regression-iteration-5\" data-toc-modified-id=\"Lasso-Regression-iteration-5-1.14\"><span class=\"toc-item-num\">1.14&nbsp;&nbsp;</span>Lasso Regression iteration 5</a></span></li><li><span><a href=\"#Lasso-Regression-Iteration-6\" data-toc-modified-id=\"Lasso-Regression-Iteration-6-1.15\"><span class=\"toc-item-num\">1.15&nbsp;&nbsp;</span>Lasso Regression Iteration 6</a></span></li><li><span><a href=\"#Elastic-Nets-Iteration-1\" data-toc-modified-id=\"Elastic-Nets-Iteration-1-1.16\"><span class=\"toc-item-num\">1.16&nbsp;&nbsp;</span>Elastic Nets Iteration 1</a></span></li><li><span><a href=\"#Some-other-methods:-KNN-Regression\" data-toc-modified-id=\"Some-other-methods:-KNN-Regression-1.17\"><span class=\"toc-item-num\">1.17&nbsp;&nbsp;</span>Some other methods: KNN-Regression</a></span></li><li><span><a href=\"#PCA-Analysis\" data-toc-modified-id=\"PCA-Analysis-1.18\"><span class=\"toc-item-num\">1.18&nbsp;&nbsp;</span>PCA Analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#PCR-Analysis\" data-toc-modified-id=\"PCR-Analysis-1.18.1\"><span class=\"toc-item-num\">1.18.1&nbsp;&nbsp;</span>PCR Analysis</a></span></li><li><span><a href=\"#PLS\" data-toc-modified-id=\"PLS-1.18.2\"><span class=\"toc-item-num\">1.18.2&nbsp;&nbsp;</span>PLS</a></span></li></ul></li><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-1.19\"><span class=\"toc-item-num\">1.19&nbsp;&nbsp;</span>Summary</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../helper/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load packages\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pandas_profiling\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "import warnings\n",
    "import math\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from statsmodels.compat import lzip\n",
    "from slugify import slugify\n",
    "from pandas.tools.plotting import table\n",
    "from plotly.offline import init_notebook_mode\n",
    "from plotly.offline import iplot\n",
    "from plotly.offline import plot\n",
    "from scipy.stats import mannwhitneyu\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics import mean_squared_error, r2_score \n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split, KFold, cross_val_predict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso, LassoCV, ElasticNetCV, ElasticNet\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.cross_decomposition import PLSRegression, PLSSVD\n",
    "from math import sqrt\n",
    "from sklearn.decomposition.pca import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "from helper import na_ratio_table\n",
    "from helper import rmsle\n",
    "from helper import reg_line\n",
    "from helper import reg_model_results_plots\n",
    "from helper import regression_model_metrics\n",
    "from helper import results_summary_to_dataframe\n",
    "from helper import lin_model\n",
    "from helper import get_model_results\n",
    "from helper import reg_model_results_plots\n",
    "from helper import resid_plot\n",
    "from helper import cross_fold\n",
    "from helper import get_baseline_for_regression\n",
    "from helper import outliers_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definitions\n",
    "\n",
    "Functions that have been created have been moved to the helper file, as they can be reused for different notebooks and in order to keep this notebook lean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_models = pd.DataFrame(columns=['Model','train_res_df','test_res_df',\n",
    "#                                    'mse_train','mse_test','rmsle_train',\n",
    "#                                    'rmsle_test', 'r2_train', 'r2_test',\n",
    "#                                    'se_train', 'se_test', 'train_obs',\n",
    "#                                    'test_obs', 'outliers_obs'])\n",
    "\n",
    "# def add_model_to_overview(model_summaries, modelname):\n",
    "#     model_dic={}\n",
    "# #     model_df=pd.DataFrame(columns=['Model','train_res_df','test_res_df',\n",
    "# #                                    'mse_train','mse_test','rmsle_train',\n",
    "# #                                    'rmsle_test', 'r2_train', 'r2_test',\n",
    "# #                                    'se_train', 'se_test', 'train_obs',\n",
    "# #                                    'test_obs', 'outliers_obs'])\n",
    "#     model_dic.update({\"Model\":modelname})\n",
    "#     model_dic.update({\"train_res_df\": model_summaries['train_res_df']})\n",
    "#     model_dic.update({\"test_res_df\":model_summaries['test_res_df']})\n",
    "#     model_dic.update({\"mse_train\":model_summaries['mse_train']})\n",
    "#     model_dic.update({\"mse_test\":model_summaries['mse_test']})\n",
    "#     model_dic.update({\"r2_train\":model_summaries['r2_train']})\n",
    "#     model_dic.update({\"r2_test\":model_summaries['rmsle_train']})\n",
    "#     model_dic.update({\"rmsle_train\":model_summaries['rmsle_test']})\n",
    "#     model_dic.update({\"rmsle_test\":model_summaries['r2_test']})\n",
    "#     model_dic.update({\"se_train\":model_summaries['se_train']})\n",
    "#     model_dic.update({\"se_test\":model_summaries['se_test']})\n",
    "#     model_dic.update({\"train_obs\":model_summaries['train_obs']})\n",
    "#     model_dic.update({\"test_obs\":model_summaries['test_obs']})\n",
    "#     model_dic.update({\"outliers_obs\":model_summaries['outliers_obs']})\n",
    "#     model_df=pd.DataFrame(model_dic,index=[0])\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "First of all we are going to cleanse the data according to what we have determined in the EDA already-> remove useless columns. Then we need to convert the Categorical variables one to numeric ones using One Hot Encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df=pd.read_csv(\"../data/house_prices_train.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df=df.drop(columns=[\"Id\", \"Street\", \"Utilities\"])\n",
    "# remove the missing record for Electrical\n",
    "df=df[df[\"Electrical\"].isna()==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAT_FEATURES =df.select_dtypes(include=\"object\").columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(CAT_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hot1encoded= pd.get_dummies(df, columns=CAT_FEATURES, prefix=CAT_FEATURES, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hot1encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all missing values, we will replace them with 0, as 0 is like they are not existent, like it is in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(na_ratio_table(df_hot1encoded)[na_ratio_table(df_hot1encoded)[\"NA_COUNT\"]>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hot1encoded=df_hot1encoded.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_hot1encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column names have some weird characters, we replace them just in case\n",
    "column_names={}\n",
    "\n",
    "df_hot1encoded.columns = df_hot1encoded.columns.str.replace(\"1\", \"One\")\n",
    "df_hot1encoded.columns = df_hot1encoded.columns.str.replace(\"2\", \"Two\")\n",
    "df_hot1encoded.columns = df_hot1encoded.columns.str.replace(\"3\", \"Three\")\n",
    "df_hot1encoded.columns = df_hot1encoded.columns.str.replace(\"4\", \"Four\")\n",
    "\n",
    "\n",
    "def better_slugify(value, seperator='-'):\n",
    "    return slugify(value).replace('-', seperator)\n",
    "\n",
    "\n",
    "for i, e in enumerate(df_hot1encoded.columns):\n",
    "    column_names[e]= better_slugify(e, seperator=\"_\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hot1encoded.rename(index=str, columns=column_names, inplace=True)#.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Training and Testingset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_COLS=df_hot1encoded.drop(columns=\"saleprice\").columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= df_hot1encoded[FEATURE_COLS]\n",
    "y= df_hot1encoded[\"saleprice\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 70% Training, 30% Testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Data with Standardscaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we also will use a StandardScaler in a second set\n",
    "\n",
    "standardizer = StandardScaler().fit(X_train)\n",
    "\n",
    "# FEATURE_COLS=df_hot1encoded.drop(columns=\"saleprice\").columns\n",
    "\n",
    "scaled_features=standardizer.transform(df_hot1encoded[FEATURE_COLS].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df=feature_df=pd.DataFrame(scaled_features, index=df_hot1encoded[FEATURE_COLS].index, columns= df_hot1encoded[FEATURE_COLS].columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled = train_test_split(feature_df.copy(), y.copy(), test_size=0.3, random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all=X.copy()\n",
    "df_all[\"saleprice\"]=y.copy()\n",
    "outliers = outliers_idx(df_all, \"saleprice\")\n",
    "outliers_df = X.loc[X.index.isin(outliers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #filter outliers \n",
    "X= X.loc[~X.index.isin(outliers)]\n",
    "y= y.loc[~y.index.isin(outliers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=X_train.copy()\n",
    "df[\"saleprice\"]=y_train.copy()\n",
    "outliers = outliers_idx(df, \"saleprice\")\n",
    "outliers_df = X_train.loc[X_train.index.isin(outliers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #filter outliers for training\n",
    "X_train= X_train.loc[~X_train.index.isin(outliers)]\n",
    "y_train= y_train.loc[~y_train.index.isin(outliers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have removed 16 outliers.\n",
    "Same process for the scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled=X_train_scaled.copy()\n",
    "df_scaled[\"saleprice\"]=y_train_scaled.copy()\n",
    "outliers_scaled = outliers_idx(df_scaled, \"saleprice\")\n",
    "outliers_df_scaled = X_train_scaled.loc[X_train_scaled.index.isin(outliers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #filter outliers for training\n",
    "X_train_scaled= X_train_scaled.loc[~X_train.index.isin(outliers)]\n",
    "y_train_scaled= y_train_scaled.loc[~y_train.index.isin(outliers)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model\n",
    "Let's start with building a baseline model, which is simply the means of all SalePrices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create Dummyregressor\n",
    "Baseline = DummyRegressor(strategy='mean')\n",
    "\n",
    "Baseline.fit(X_train, y_train)\n",
    "Baseline.predict(X_test)\n",
    "# Mean Accuracy ausgeben\n",
    "print('Baseline MSE: %.2f' % mean_squared_error(Baseline.predict(X_test), y_test))\n",
    "baseline_mse=mean_squared_error(Baseline.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our baseline model, which uses the mean of all SalePrices of the Training set gives us a MSE of 5,548,069,835.64, which seems **very** high. Using different statistcal and machine learning methods we want to improve this score now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsle(Baseline.predict(X_test),y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinary Least Square Regression Model (Iteration 1)\n",
    "- all variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's start with a simple model and no further adjustments, we just put in all variables we have\n",
    "ols_model = sm.OLS(y_train, X_train).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before, we might run into some multicollinearity problems. Let's check the results of the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=ols_model.predict(X_train)\n",
    "mean_squared_error(y_pred, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ols_model.get_prediction(X_test)\n",
    "# ols_mse=mean_squared_error(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_pred=ols_model.predict(X_test)\n",
    "mean_squared_error(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsle(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSE for both training AND testing set are very high. The model does not perform very well, although the RÂ² and the F2-Score are very high. As it is mentioned in the summary, we suspect that we have a problem with multicollinearity, worsening our model significantly. We have too many predictors, of which many also have correlations. \n",
    "\n",
    "\n",
    "We need to reduce the dimensions. However, a challenge that we might have is, that too many explanatory variabbles could be chosen and subsets of variables might be highly correlated to one another. Regression coefficients might get very unstable and we might overfit the models. Thus we need different ways to reduce the number of dimensions and resolve issues with multicollinearity. \n",
    "First, we check the coefficents and p-values for each attributes. Let's have a look at those with p-values higher than the thumbrule 0.05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_attr= results_summary_to_dataframe(ols_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_attr= ols_attr[ols_attr.pvals<=0.05]#.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ols_attr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would have 47 attributes for which we could reject the 0-Hypothesis, based on the OLS. Let's try building a model using these attributes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinary Least Square Regression Model (Iteration 2)\n",
    "- remove iteratively non-significant variables based on p-values of variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_model_summaries={}\n",
    "y_pred_name = \"saleprice_predicted\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ols_model_2 = sm.OLS(y_train, X_train).fit()\n",
    "\n",
    "while results_summary_to_dataframe(ols_model_2)[\"pvals\"].apply(lambda x: x >= 0.05).any():\n",
    "    ols_model_sign_attr=[]\n",
    "    for i, e in enumerate(results_summary_to_dataframe(ols_model_2)[\"pvals\"]):\n",
    "        if e <=0.05:\n",
    "            ols_model_sign_attr.append(results_summary_to_dataframe(ols_model_2).iloc[i][\"attributes\"])\n",
    "    ols_model_2 = sm.OLS(y_train, X_train[ols_model_sign_attr]).fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test results\n",
    "reg_test = ols_model_2.predict(X_test[ols_model_sign_attr])\n",
    "\n",
    "# get results\n",
    "reg_train_results_df = pd.DataFrame({\"saleprice\": y_train.copy()})\n",
    "reg_train_results_df[y_pred_name] = ols_model_2.fittedvalues\n",
    "reg_train_results_df['set'] = 'TRAIN'\n",
    "\n",
    "reg_test_results_df = pd.DataFrame({\"saleprice\": y_test.copy()})\n",
    "reg_test_results_df[y_pred_name] = reg_test\n",
    "reg_test_results_df['set'] = 'TEST'\n",
    "\n",
    "# metrics\n",
    "baseline_val, baseline_mse, baseline_rmsle = get_baseline_for_regression(\n",
    "    X_train=X_train,\n",
    "    y_train=reg_train_results_df[\"saleprice\"],\n",
    "    X_test=X_test,\n",
    "    y_test=reg_test_results_df[\"saleprice\"])\n",
    "\n",
    "mse_train, r2_train, adj_r2_train, st_error_train = regression_model_metrics(\n",
    "    y=reg_train_results_df[\"saleprice\"],\n",
    "    y_predicted=reg_train_results_df[y_pred_name],\n",
    "    X=X_train)\n",
    "\n",
    "mse_test, r2_test,adj_r2_test, st_error_test = regression_model_metrics(\n",
    "    y=reg_test_results_df[\"saleprice\"],\n",
    "    y_predicted=reg_test_results_df[y_pred_name],\n",
    "    X=X_test)\n",
    "\n",
    "rmsle_train=rmsle(ytest=reg_train_results_df[\"saleprice\"], \n",
    "                    ypred=reg_train_results_df[y_pred_name])\n",
    "rmsle_test=rmsle(ytest=reg_test_results_df[\"saleprice\"], \n",
    "                     ypred=reg_test_results_df[y_pred_name])\n",
    "\n",
    "ols_model_summaries = {'y_pred_name': y_pred_name,\n",
    "                       'train_res_df': reg_train_results_df,\n",
    "                       'test_res_df': reg_test_results_df,\n",
    "                       'baseline_mse': baseline_mse,\n",
    "                       'baseline_val': baseline_val,\n",
    "                       'baseline_rmsle':baseline_rmsle,\n",
    "                       'mse_train': mse_train,\n",
    "                       'mse_test': mse_test,\n",
    "                       'rmsle_train': rmsle_train,\n",
    "                       'rmsle_test':rmsle_test,\n",
    "                       'r2_train': r2_train,\n",
    "                       'adj_r2_train': adj_r2_train,\n",
    "                       'r2_test': r2_test,\n",
    "                       'adj_r2_test': adj_r2_test,\n",
    "                       'se_train': st_error_train,\n",
    "                       'se_test': st_error_test,\n",
    "                       'model_report': ols_model_2.summary(),\n",
    "                       'train_obs': reg_train_results_df.shape[0],\n",
    "                       'test_obs': reg_test_results_df.shape[0]}\n",
    "# put all this in a function in case, i want to repeat this analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model_results_plots(model_summaries=ols_model_summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are okay, the MSE is definitely smaller than are benchmark/baseline. if we look at the plot that compares Actual vs. Predicted, the data points are quite close to our regression line, which indicates a good prediction. If the data points were exactly on the regression line, we would have a perfect regression. It is worse than the model including ALL variables though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residuals vs. Fitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_plot(ols_model_2, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The residuals are close to but not exactly spread around a horizontal line. There seems to be some non-linear relationships, that the model didn't capture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "name = ['Jarque-Bera', 'Chi^2 two-tail prob.', 'Skew', 'Kurtosis']\n",
    "test = sm.stats.jarque_bera(ols_model_2.resid)\n",
    "lzip(name, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JB- Number is very high indicating that the residuals are not normally distributed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sm.qqplot(ols_model_2.get_influence().resid_studentized_internal);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_model_to_overview(ols_model_summaries, \"OLS - removal of attributes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinary Least Square Regression Model (Iteration 3)\n",
    "- use scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_model_3 = sm.OLS(y_train_scaled, X_train_scaled).fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ols_model_3, ols_model_3_summary, sign_attr_3=lin_model(y_train_scaled, X_train_scaled, y_test_scaled, X_test_scaled, reg_model=\"OLS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reg_model_results_plots(model_summaries=ols_model_3_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the standardized data results in a much worse result. WHY? We also have negative values, which should not be possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residuals vs. Fitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fitted values\n",
    "model_fitted_y_2 = ols_model_3.fittedvalues;\n",
    "# Basic plot\n",
    "plot = sns.residplot(model_fitted_y_2, y_train, lowess=True, \n",
    "                     scatter_kws={'alpha': 0.5}, \n",
    "                     line_kws={'color': 'red', \n",
    "                               'lw': 1, 'alpha': 0.8});\n",
    "\n",
    "plot.set_title('Residuals vs Fitted');\n",
    "plot.set_xlabel('Fitted values');\n",
    "plot.set_ylabel('Residuals');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinary Least Square - Cross Validation\n",
    "- using the model from second iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors=\" + \".join(ols_model_sign_attr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1\n",
    "results = cross_fold(df_hot1encoded, response='saleprice', predictors=predictors)\n",
    "display(results)\n",
    "print(f'MSE Mean: {results[\"MSE\"].mean():.2f}')\n",
    "print(f'RMSLE Mean: {results[\"RMSLE\"].mean():.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinary Least Square Iteration 4\n",
    "- use natural log for the dependent variable to approach a normal distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_log= np.log(y_train)\n",
    "y_test_log= np.log(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_model_4, ols_model_summary_4, ols_sign_attr_4=lin_model(y_train_log, X_train, y_test_log, X_test, reg_model=\"OLS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model_results_plots(model_summaries=ols_model_summary_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_plot(ols_model_4, y_train_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.qqplot(ols_model_4.get_influence().resid_studentized_internal);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_model_5, ols_model_summary_5, ols_sign_attr_5=lin_model(y_train, X_train, y_test, X_test, reg_model=\"OLS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Least Square Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gls_model, gls_model_summary, gls_sign_attr=lin_model(y_train, X_train, y_test, X_test, reg_model=\"GLS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reg_model_results_plots(model_summaries=gls_model_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residuals vs. Fitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic plot\n",
    "plot = sns.residplot(gls_model.fittedvalues, y_train, lowess=True, \n",
    "                     scatter_kws={'alpha': 0.5}, \n",
    "                     line_kws={'color': 'red', \n",
    "                               'lw': 1, 'alpha': 0.8});\n",
    "\n",
    "plot.set_title('Residuals vs Fitted');\n",
    "plot.set_xlabel('Fitted values');\n",
    "plot.set_ylabel('Residuals');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "name = ['Jarque-Bera', 'Chi^2 two-tail prob.', 'Skew', 'Kurtosis']\n",
    "test = sm.stats.jarque_bera(gls_model.resid)\n",
    "lzip(name, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JB- Number is very high indicating that the residuals are not normally distributed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression Iteration 1\n",
    "Lasso helps us to reduce dimensions and drop unimportant attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso1 = Lasso()\n",
    "lasso1.set_params(alpha=0.5)\n",
    "lasso1.fit(X_train, y_train)\n",
    "print(lasso1.score(X_train, y_train))\n",
    "print(lasso1.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=lasso1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lasso_coef1=pd.Series(lasso1.coef_, index=X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(lasso_coef1[(lasso_coef1==0)].index.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can see plenty 14 attributes which have coefficents of zero. Those are identified as irrelevant by the lasso regression and we are going to remove those from the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make function to get number of irrelevant attributes\n",
    "def get_irr_attr_lasso(lasso_model, X_train):\n",
    "    lasso_coef=pd.Series(lasso_model.coef_, index=X_train.columns)\n",
    "    irr_attr_lasso=lasso_coef[(lasso_coef==0)].index.tolist()\n",
    "    return irr_attr_lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make function to get number of irrelevant attributes\n",
    "def get_attr_lasso(lasso_model, X_train):\n",
    "    lasso_coef=pd.Series(lasso_model.coef_, index=X_train.columns)\n",
    "    attr_lasso=lasso_coef[(lasso_coef!=0)].index.tolist()\n",
    "    return attr_lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_irr_attr_lasso(lasso1, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_attr=lasso_coef1[lasso_coef1!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_attr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 226 attributes considered as important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_attr=lasso_attr.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso()\n",
    "lasso.set_params(alpha=0.5)\n",
    "lasso.fit(X_train[lasso_attr], y_train)\n",
    "print(lasso.score(X_train[lasso_attr], y_train))\n",
    "print(lasso.score(X_test[lasso_attr], y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=lasso.predict(X_test[lasso_attr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_test, r2_test, adj_r2_test, st_error_test = regression_model_metrics(y_test, y_pred, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso, lasso_summaries =  get_model_results(lasso,y_train, X_train[lasso_attr], y_test, X_test[lasso_attr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reg_model_results_plots(lasso_summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression Iteration 2\n",
    "- iteratively removing irrelevant attributes based on lasso/coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lasso2 = Lasso()\n",
    "lasso2.set_params(alpha=0.5)\n",
    "lasso2.fit(X_train, y_train)\n",
    "\n",
    "X_train_lasso=X_train.copy()\n",
    "while len(get_irr_attr_lasso(lasso2, X_train_lasso))>0:\n",
    "    lasso_attr=get_attr_lasso(lasso2, X_train_lasso)\n",
    "    X_train_lasso=X_train[lasso_attr]\n",
    "    lasso2.fit(X_train_lasso, y_train)\n",
    "\n",
    "\n",
    "print(lasso2.score(X_train[lasso_attr], y_train))\n",
    "print(lasso2.score(X_test[lasso_attr], y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(get_irr_attr_lasso(lasso2, X_train_lasso))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(get_attr_lasso(lasso2, X_train_lasso))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso2, lasso_summaries2 =  get_model_results(lasso2,y_train, X_train[lasso_attr], y_test, X_test[lasso_attr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model_results_plots(lasso_summaries2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This didn't improve the model at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression Iteration 3\n",
    "- Using various alphas and comparing results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_r2(lambdas, X_train, y_train, X_test, y_test):\n",
    "    '''\n",
    "    Takes in a list of lambdas. \n",
    "    Outputs a dataframe containing the r2 of lasso regressions \n",
    "    from each lambda for training and testing data set.\n",
    "    \n",
    "    '''\n",
    "    columns_list=[\"lambda\", \"R2_training\", \"R2_testing\"]\n",
    "    r_scores=pd.DataFrame(columns=columns_list)\n",
    "    \n",
    "    for l in lambdas:\n",
    "        lasso=Lasso()\n",
    "        lasso.set_params(alpha=l)\n",
    "        lasso.fit(X_train, y_train)\n",
    "        R2_train=lasso.score(X_train, y_train)\n",
    "        R2_test=lasso.score(X_test, y_test)\n",
    "        row=pd.DataFrame([[l,R2_train, R2_test]], columns=columns_list)\n",
    "        r_scores=r_scores.append(row)\n",
    "    return r_scores\n",
    "\n",
    "def plot_lasso_lambdas(r_scores):\n",
    "    plt.figure(figsize=(15,8)) \n",
    "    ax=plt.plot( 'lambda', 'R2_training', data=r_scores, marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=4)\n",
    "    ax=plt.plot( 'lambda', 'R2_testing', data=r_scores, marker='o', markerfacecolor='red', markersize=12, color='tomato', linewidth=4)\n",
    "#     plt.xscale(\"log\")\n",
    "    plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_min = 0.05 \n",
    "l_max = 20\n",
    "l_num = 20\n",
    "lambdas = np.linspace(l_min,l_max, l_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_lasso_lambdas(lasso_r2(lambdas, X_train, y_train, X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_r2_results=lasso_r2(lambdas, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_r2_results=lasso_r2_results.reset_index().drop(columns=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_r2_results[\"R2_testing\"].idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lasso_r2_results.ix[lasso_r2_results[\"R2_testing\"].idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso3=Lasso()\n",
    "lasso3.set_params(alpha=11.6)\n",
    "lasso3.fit(X_train[lasso_attr], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso3, lasso_summaries3 =  get_model_results(lasso3,y_train, X_train[lasso_attr], y_test, X_test[lasso_attr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reg_model_results_plots(lasso_summaries3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly overfitted. MSE for testing set is much higher than for testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(get_attr_lasso(lasso3, X_train[lasso_attr]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_attr_lasso(lasso3, X_train[lasso_attr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression Iteration 4\n",
    "used normalized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso=Lasso()\n",
    "lasso.set_params(alpha=11.6)\n",
    "lasso.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_r2_results_2=lasso_r2(lambdas, X_train_scaled[lasso_attr], y_train, X_test_scaled[lasso_attr], y_test)\n",
    "lasso_r2_results_2=lasso_r2_results_2.reset_index().drop(columns=\"index\")\n",
    "lasso_r2_results_2.ix[lasso_r2_results_2[\"R2_testing\"].idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lasso_lambdas(lasso_r2(lambdas, X_train_scaled[lasso_attr], y_train, X_test_scaled[lasso_attr], y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso4=Lasso()\n",
    "lasso4.set_params(alpha=20)\n",
    "lasso4.fit(X_train_scaled[lasso_attr], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso4, lasso_summaries_4 =  get_model_results(lasso4,y_train, X_train_scaled[lasso_attr], y_test, X_test_scaled[lasso_attr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reg_model_results_plots(lasso_summaries_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like before it got worse. Especially the adj. R2 is significantly lower. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression iteration 5 \n",
    "\n",
    "Crossvalidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_5 = LassoCV(cv=10, random_state=0, normalize=True).fit(X, y)\n",
    "lasso_5.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_5, lasso_summaries_5 =  get_model_results(lasso_5,y, X, y, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reg_model_results_plots(lasso_summaries_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time using the normalization parameter provides better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(get_attr_lasso(lasso_5, X_train))\n",
    "display(len(get_attr_lasso(lasso_5, X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_irr_attr_lasso(lasso_5, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_5_coef = pd.Series(lasso_5.coef_, index = X_train.columns).sort_values()\n",
    "lasso_5_coef = lasso_5_coef[lasso_5_coef!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_5_coef.head(10).plot(kind=\"barh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_5_coef.tail(10).plot(kind=\"barh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression Iteration 6\n",
    "- Cross validation\n",
    "- optimization of alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_min = 0.05 \n",
    "l_max = 20\n",
    "l_num = 20\n",
    "lambdas = np.linspace(l_min,l_max, l_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso6 = LassoCV(cv=10, random_state=0, alphas=lambdas, fit_intercept=True).fit(X, y)\n",
    "lasso6.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso6, lasso_summaries_6 =  get_model_results(lasso6,y, X, y, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model_results_plots(lasso_summaries_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso6.alpha_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best alpha: 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using an intercept and an optimization of the alphas results in quite a good result. The RMSLE is very low in comparison and also the R2 is good. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Nets Iteration 1\n",
    "Especially, when you have **a lot** of attributes. Elastic Nets are especially good at dealing with correlations between parameters, which we strongly assume is the case here.  For our example, we have over 200 attributes. This is what we would also consider as a lot. This is why elastic nets sound quite promising for our example. \n",
    "Elastic Nets is a combination of Lasso and Ridge Regression. \n",
    "\n",
    "We will use CV for this only. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [0.0005, 0.001, 0.01, 0.03, 0.05, 0.1]\n",
    "l1_ratios = [ .1, .5, .7, .8, .9, .95, .99, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ElasticNet1 = ElasticNetCV(cv=5, random_state=0, l1_ratio =l1_ratios, alphas=lambdas, fit_intercept=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ElasticNet1.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ElasticNet1, ElasticNet1_summaries =  get_model_results(ElasticNet1,y, X, y, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model_results_plots(ElasticNet1_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"L1 ratio: \", ElasticNet1.l1_ratio_)\n",
    "print(\"Alpha: \", ElasticNet1.alpha_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The L1 ration is close to one, which means that this is very close to the Lasso regression. Still the Lasso yield better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "el_net_coef = pd.Series(ElasticNet1.coef_, index = X_train.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "el_net_coef=el_net_coef.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "el_net_coef.head(10).plot(kind=\"barh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "el_net_coef.tail(10).plot(kind=\"barh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Elastic Net picked \" + str(sum(el_net_coef != 0)) + \" variables and eliminated the other \" +  str(sum(el_net_coef == 0)) + \" variables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 4 Variables were eliminated. \n",
    "--> not really helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some other methods: KNN-Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsRegressor(n_neighbors=n_neighbors)\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn, knn_summaries =  get_model_results(knn,y_train, X_train, y_test, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model_results_plots(knn_summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the results are not very good. The model particularly does not handle values in the upper end very well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_values=X.values\n",
    "y_values=y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we do not know how many components to choose, we select \"None\"\n",
    "# as number of components to get an overview over the variance in the dataset\n",
    "pca= PCA(n_components=None)\n",
    "X_values_scaled= StandardScaler().fit_transform(X_values)\n",
    "X_values_scaled= pca.fit_transform(X_values_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We check our explained variance ratio in the dataset by adding each \n",
    "# consecutive principal component\n",
    "np.cumsum(np.round(pca.explained_variance_ratio_,decimals=4)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For better understanding we plot the variance curve.\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This curve quantifies how much of the total, 244-dimensional variance is contained within the first $N$ components. We see that the first ~120 components contain approximately 80% of the variance, while you need around 230 components to describe close to 100% of the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCR Analysis\n",
    "We build a function to perform a PCR. This concept combines a PCA and a logistic regression. \n",
    "Using Crossvalidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pcr(X,y,pc):\n",
    "    pca3 = PCA()\n",
    "     \n",
    "    X_r= StandardScaler().fit_transform(X)\n",
    "    X_r= pca.fit_transform(X)\n",
    "    \n",
    "    # Run PCA producing the reduced variable Xreg and select the first pc components\n",
    "    Xreg = pca.fit_transform(X_r)[:,:pc]\n",
    " \n",
    "    ''' Step 2: regression on selected principal components'''\n",
    " \n",
    "    # Create linear regression object\n",
    "    regr = LinearRegression()\n",
    "    \n",
    "    # Fit\n",
    "    regr.fit(Xreg, y)\n",
    " \n",
    "    # prediction of calibration\n",
    "    y_c = regr.predict(Xreg)\n",
    " \n",
    "    # Cross-validation\n",
    "    y_cv = cross_val_predict(regr, Xreg, y, cv=10)\n",
    " \n",
    "    # Calculate scores for calibration and cross-validation\n",
    "    score_c = r2_score(y, y_c)\n",
    "    score_cv = r2_score(y, y_cv)\n",
    " \n",
    "    # Calculate mean square error for calibration and cross validation\n",
    "    mse_c = mean_squared_error(y, y_c)\n",
    "    mse_cv = mean_squared_error(y, y_cv)\n",
    " \n",
    "    return(regr, y_cv, score_c, score_cv, mse_c, mse_cv, Xreg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcr, predicted, r2r, r2cv, mser, mscv, x_pcr=pcr(X_values,y_values, pc=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcr, pcr_summaries = get_model_results(pcr,y,x_pcr, y, x_pcr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model_results_plots(pcr_summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLS\n",
    "\n",
    "Partial Least Squares:\n",
    "\n",
    "* Technique similar to Principal Component Analysis (PCA).\n",
    "* Decomposes both the design matrix X and response vector/matrix Y (can have multiple responses at once) like in PCA and then performs regression between the obtained scores matrix and the eigenvectors.\n",
    "* The idea is to obtain decomposition of X and Y to be done by taking information from each other into account.\n",
    "* The most important variables can be selected by the VIP score, which measures explicative power of predictor variables with respect to the response variable in all selected components. Normally, the average of the squared values of the VIPs is equal to 1. The criterion of VIP value with greater than 1 is then typically used as a cutoff point for variable selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_values=X_train.values\n",
    "y_train_values=y_train.values\n",
    "X_test_values=X_test.values\n",
    "y_test_values=y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse=[]\n",
    "component = np.arange(1,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in component:\n",
    "    pls1= PLSRegression(n_components=i)\n",
    "    pls1.fit(X_train,y_train)\n",
    "    y_pred = pls1.predict(X_test)\n",
    "    \n",
    "    mse_p = mean_squared_error(y_test,y_pred)\n",
    "    mse.append(mse_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmin(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msemin = np.argmin(mse)\n",
    "print(\"Suggested number of components: \", msemin+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context(('ggplot')):\n",
    "     plt.plot(component, np.array(mse), '-v', color = 'blue', mfc='blue')\n",
    "     plt.plot(component[msemin], np.array(mse)[msemin], 'P', ms=10, mfc='red')\n",
    "     plt.xlabel('Number of PLS components')\n",
    "     plt.ylabel('MSE')\n",
    "     plt.title('PLS')\n",
    "     plt.xlim(xmin=-1)\n",
    " \n",
    "     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pls = PLSRegression(n_components=msemin+1)\n",
    "pls.fit(X_train, y_train)\n",
    "Y_pred = pls.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pls, pls_summaries = get_model_results(pls,y_train,X_train, y_test, X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model_results_plots(pls_summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "Even though we didn't quite reach the Leaderboard, our results weren't too bad! With some more time and tweaking, we would definitely have been able to improve more."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "758px",
    "left": "391px",
    "top": "191px",
    "width": "332px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 701,
   "position": {
    "height": "723px",
    "left": "1044px",
    "right": "20px",
    "top": "92px",
    "width": "798px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
