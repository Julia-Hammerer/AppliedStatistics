{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Analysis on Car Seats Data\n",
    "\n",
    "We will divide the house prices in two categories and then try to predict those. The categories represents the target that the Car Seat Company is trying to reach and if that has been reached.\n",
    "\n",
    "Author: Julia Hammerer, Vanessa Mai \n",
    "Last Changes: 10.12.2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../helper/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split, KFold\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import itertools\n",
    "from scipy import interp\n",
    "from sklearn import svm\n",
    "\n",
    "from helper import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"../data/Carseats.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\"Unnamed: 0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sales_target(row):\n",
    "    if row[\"Sales\"]<=7.49:\n",
    "        return \"No\"\n",
    "    else:\n",
    "        return \"Yes\"\n",
    "\n",
    "\n",
    "df[\"AboveTarget\"]= df.apply(lambda row: Sales_target(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"Sales\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have to encode our categorical data\n",
    "cols_to_transform= df.drop(columns=[\"AboveTarget\"]).select_dtypes(include='object').columns.values\n",
    "df_hot1encoded= pd.get_dummies(df, columns=cols_to_transform, prefix=cols_to_transform, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_hot1encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_hot1encoded.head())\n",
    "print(df_hot1encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureCols=df_hot1encoded.columns.tolist()\n",
    "featureCols.remove(\"AboveTarget\")\n",
    "\n",
    "targetCol=df_hot1encoded[\"AboveTarget\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= df_hot1encoded[featureCols].values\n",
    "y= df_hot1encoded[\"AboveTarget\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scaler= StandardScaler()\n",
    "scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison\n",
    "in order to compare the models, we will save the models and the CV-results in a list:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_models = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Iteration 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe1 = Pipeline([('stdSc', StandardScaler()),\n",
    "                 ('clf', LogisticRegression(C=0.1,random_state=1)) \n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=pipe1.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Accuracy: %.3f' % pipe1.score(X_train, y_train)) \n",
    "print('Test Accuracy: %.3f' % pipe1.score(X_test, y_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class_names = ('Below Target', 'Above Target')\n",
    "\n",
    "confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "plot_confusion_matrix(confusion_mat, normalize=False, classes=class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Accuracy:       \",accuracy_score(y_test, y_pred))\n",
    "print (\"Precision:      \",precision_score(y_test, y_pred,average=None))\n",
    "print (\"Recall:         \",recall_score(y_test, y_pred,average=None))\n",
    "print (\"F1-Score:       \",f1_score(y_test, y_pred,average=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results for the `Middle Class`is better than the others. This is expected, since there are more samples for that class. Overall, the results are somewhat good. Let's try optimizing it all a bit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Iteration 2\n",
    "Gridsearch plus CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe1.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since newton-cg and saga only accepts l2 penalty, we will only use that in the gridsearch\n",
    "penalties=[\"l2\"]\n",
    "c=np.logspace(-4, 0, 40)\n",
    "solvers = [\"newton-cg\", \"sag\", \"saga\", \"lbfgs\"]\n",
    "\n",
    "param_grid_lr={'clf__penalty' : penalties,\n",
    "            'clf__C' :c,\n",
    "              'clf__solver': solvers}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_randSearch = RandomizedSearchCV(estimator=pipe1, param_distributions=param_grid_lr,scoring='accuracy',cv=10, n_iter=8)\n",
    "lr_randSearch.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Configuration found by Grid Search\")\n",
    "print(lr_randSearch.best_score_)\n",
    "print(lr_randSearch.best_params_)\n",
    "print(lr_randSearch.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe2=lr_randSearch.best_estimator_\n",
    "pipe2.fit(X_train, y_train)\n",
    "y_pred=pipe2.predict(X_test)\n",
    "confusion_mat=confusion_matrix(y_test, y_pred)\n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "print(\"Recall: \",recall_score(y_test, y_pred,average=None))\n",
    "print(\"Precision: \",precision_score(y_test, y_pred,average=None))\n",
    "print(\"F1: \",f1_score(y_test, y_pred,average=None))\n",
    "print(confusion_mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add model to summary\n",
    "opt_models.append((\"Log Reg\", pipe2.steps[1][1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall is not as good as the other scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score=cross_val_score(pipe2, X, y, scoring=\"accuracy\", cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score.mean()\n",
    "print(\"Mean accuracy: %5.2f (+/-%5.2f) \"%(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, title='Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same results as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance (LR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LRImp=pd.DataFrame(index=featureCols, columns=[\"LR\"])\n",
    "LRImp[\"LR\"]=pipe2.steps[1][1].coef_[0]\n",
    "feature_imp=LRImp.sort_values(by=\"LR\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp.sort_values(by=\"LR\", ascending=True).plot(kind=\"barh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Iteration 3\n",
    "- used subset of feature based on previous Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_features=feature_imp[abs(feature_imp[\"LR\"])>0.05].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe3 = Pipeline([('stdSc', StandardScaler()),\n",
    "                 ('clf', LogisticRegression(C=0.1,random_state=1)) \n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since newton-cg and saga only accepts l2 penalty, we will only use that in the gridsearch\n",
    "penalties=[\"l2\"]\n",
    "c=np.logspace(-4, 0, 40)\n",
    "solvers = [\"newton-cg\", \"sag\", \"saga\", \"lbfgs\"]\n",
    "\n",
    "param_grid_lr={'clf__penalty' : penalties,\n",
    "            'clf__C' :c,\n",
    "              'clf__solver': solvers}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr_randSearch2 = RandomizedSearchCV(estimator=pipe3, param_distributions=param_grid_lr,scoring='accuracy',cv=10, n_iter=8)\n",
    "lr_randSearch2.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Configuration found by Grid Search\")\n",
    "print(lr_randSearch2.best_score_)\n",
    "print(lr_randSearch2.best_params_)\n",
    "print(lr_randSearch2.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe3=lr_randSearch.best_estimator_\n",
    "pipe3.fit(X_train, y_train)\n",
    "y_pred=pipe3.predict(X_test)\n",
    "confusion_mat=confusion_matrix(y_test, y_pred)\n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "print(\"Recall: \",recall_score(y_test, y_pred,average=None))\n",
    "print(\"Precision: \",precision_score(y_test, y_pred,average=None))\n",
    "print(\"F1: \",f1_score(y_test, y_pred,average=None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, title='Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model did not improve much. Model can differentiate between low and high very well. For distinguishing between middle and low or middle and upper it has a little more difficulty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_svm1 = Pipeline([('stdSc', StandardScaler()),\n",
    "                 ('clf', svm.SVC() )\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_svm1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=pipe_svm1.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Accuracy: %.3f' % pipe_svm1.score(X_train, y_train)) \n",
    "print('Test Accuracy: %.3f' % pipe_svm1.score(X_test, y_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_mat = confusion_matrix(y_test, y_pred)\n",
    "plot_confusion_matrix(cnf_mat,class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Accuracy:       \",accuracy_score(y_test, y_pred))\n",
    "print (\"Precision:      \",precision_score(y_test, y_pred,average=None))\n",
    "print (\"Recall:         \",recall_score(y_test, y_pred,average=None))\n",
    "print (\"F1-Score:       \",f1_score(y_test, y_pred,average=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM iteration 2\n",
    "Let's try optimizing this model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_svm1.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_svm = [\n",
    " {'clf__C': [1, 10, 100, 1000], 'clf__kernel': ['linear']},\n",
    " {'clf__C': [1, 10, 100, 1000], 'clf__gamma': [0.001, 0.0001], 'clf__kernel': ['rbf']},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_GridSearch = GridSearchCV(estimator=pipe_svm1, param_grid=param_grid_svm,scoring='accuracy',cv=10)\n",
    "svm_GridSearch.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Configuration found by Grid Search\")\n",
    "print(svm_GridSearch.best_score_)\n",
    "print(svm_GridSearch.best_params_)\n",
    "print(svm_GridSearch.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_svm2=svm_GridSearch.best_estimator_\n",
    "pipe_svm2.fit(X_train, y_train)\n",
    "y_pred=pipe_svm2.predict(X_test)\n",
    "confusion_mat=confusion_matrix(y_test, y_pred)\n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "print(\"Recall: \",recall_score(y_test, y_pred,average=None))\n",
    "print(\"Precision: \",precision_score(y_test, y_pred,average=None))\n",
    "print(\"F1: \",f1_score(y_test, y_pred,average=None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, title='Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add model to summary\n",
    "opt_models.append((\"SVM\", pipe_svm2.steps[1][1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seed=7\n",
    "# evaluate each model in turn\n",
    "opt_results = []\n",
    "opt_names = []\n",
    "scoring = 'accuracy'\n",
    "for name, model in opt_models:\n",
    "    kfold = KFold(n_splits=10, random_state=seed)\n",
    "    cv_results = cross_val_score(model, X, y, cv=kfold, scoring=scoring)\n",
    "    opt_results.append(cv_results)\n",
    "    opt_names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "# boxplot algorithm comparison\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(opt_results)\n",
    "ax.set_xticklabels(opt_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression performs much better"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "192px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
